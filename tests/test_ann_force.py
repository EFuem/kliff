from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
import tensorflow as tf
from openkim_fit.dataset import DataSet
from openkim_fit.descriptor import Descriptor
from tensorflow.contrib.layers import fully_connected, xavier_initializer, variance_scaling_initializer
import openkim_fit.ann as ann

# Check that the forces and energys are the same with the KIM model. (No centering
# and normalizing. )
# 1. run this script
# 2. copy `ann_kim.params' generated by this script to the KIM Carbon model, and make.
# 3. run LAMMPS with input `lammps_graphene.in' to generate the dump file.
#    (located at bop  /usr/scratch/wenxx151/Documents/lmp_playground/debug_ann)
# 4. compare the stdout of this script and the LAMMPS dump file.
# 5. Agreement between models you want to check when this test does not pass.
# 1) Both use the same STRUCT.
# 2) KIM model compute descriptor and forces in two different places, so ensure
#    modifications are made to both places.
# 3) This script will write the not normalized descriptor values to file `debug_descriptor.txt`.
#    It can be compared with the one generated by the model driver. TO enable it, search `TODO`
#    in ANNImplementation.hpp.
# 4) We can write the normalized descirptor `debug_descriptor_after_normalization.txt'
#    if `use_welford' is not selected. You can uncomment that in `numpy_mean_and_std()' of ann.py.

# set a global random seed
tf.set_random_seed(1)


##############################
# settings
#DO_NORMALIZE = False
DO_NORMALIZE = True
#STRUCT = 'bulk'
STRUCT = 'bilayer'
##############################


# create Descriptor
cutfunc = 'cos'
cutvalue = {'C-C':5.}
#cutvalue = {'Mo-Mo':5., 'Mo-S':5., 'S-S':5.}
desc_params = {
  'g1': None,
  'g2': [{'eta':0.1, 'Rs':0.2},
         {'eta':0.3, 'Rs':0.4}],
  'g3': [{'kappa':0.1},
         {'kappa':0.2},
         {'kappa':0.3}],
  'g4': [{'zeta':0.1, 'lambda':0.2, 'eta':0.01},
         {'zeta':0.3, 'lambda':0.4, 'eta':0.02}],
  'g5': [{'zeta':0.11, 'lambda':0.22, 'eta':0.011},
         {'zeta':0.33, 'lambda':0.44, 'eta':0.022}]
                }

desc = Descriptor(desc_params, cutfunc, cutvalue,  cutvalue_samelayer=cutvalue, debug=True)
num_desc = desc.get_num_descriptors()


# read config and reference data
tset = DataSet()
tset.read('./training_set/graphene_bilayer_1x1.xyz')
#tset.read('./training_set/training_set_mos2_small_config_4/mos2_2x2_a3.0.xyz')
configs = tset.get_configs()


# preprocess data to generate tfrecords
DTYPE = tf.float64
train_name, _ = ann.convert_to_tfrecord(configs, desc,
    size_validation = 0, directory='/tmp',
    do_generate=True, do_normalize=DO_NORMALIZE, do_shuffle=False,
    use_welford=False, fit_forces=True, structure=STRUCT, dtype=DTYPE)

# read data from tfrecords into tensors
dataset = ann.read_tfrecord(train_name, fit_forces=True, dtype=DTYPE)

# number of epoches
iterator = dataset.make_one_shot_iterator()


# create NN
initializer = ann.weight_decorator(xavier_initializer(dtype=DTYPE))
size = 20
subloss = []

name,num_atoms_by_species,weight,gen_coords,energy_label,atomic_coords, dgen_datomic_coords,forces_label = iterator.get_next(    )

in_layer = ann.input_layer_given_data(atomic_coords, gen_coords,
    dgen_datomic_coords, num_descriptor=num_desc)

hidden1 = fully_connected(in_layer, size, activation_fn=tf.nn.tanh,
   weights_initializer=initializer,
   biases_initializer=tf.zeros_initializer(dtype=DTYPE), scope='hidden1')

hidden2 = fully_connected(hidden1, size, activation_fn=tf.nn.tanh,
   weights_initializer=initializer,
   biases_initializer=tf.zeros_initializer(dtype=DTYPE), scope='hidden2')

output = fully_connected(hidden2, 1, activation_fn=None,
   weights_initializer=initializer,
   biases_initializer=tf.zeros_initializer(dtype=DTYPE), scope='output')


# energy and forces
energy = tf.reduce_sum(output)
forces = tf.gradients(output, atomic_coords)[0]  # tf.gradients return a LIST of tensors

weights, biases = ann.get_weights_and_biases(['hidden1', 'hidden2', 'output'])


with tf.Session() as sess:

  # init global vars
  init_op = tf.global_variables_initializer()
  sess.run(init_op)

  en,fo = sess.run([energy, forces])
  print('energy:', en)
  print('forces:')
  for i,f in enumerate(fo):
    print('{:13.5e}'.format(f), end='')
    if i%3==2:
      print()

  # output results to a KIM model
  w,b = sess.run([weights, biases])
  ann.write_kim_ann(desc, w, b, tf.nn.tanh, dtype=tf.float64)

